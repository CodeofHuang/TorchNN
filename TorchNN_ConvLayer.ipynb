{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/zDX6/j7o7Tmb+gH+dkC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sardear/TorchNN/blob/main/TorchNN_ConvLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Torch Conv2d:example checkt weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "c1 = nn.Conv2d(3,1,3, stride=1) #input:3,output:1,kernel:3\n",
        "# c1 = nn.Conv2d(3,2,3, stride=1) #input:3,output:1,kernel:3\n",
        "print(\"C1 weights:\",c1.weight.shape) #show weights\n",
        "print(c1.weight)\n",
        "\n",
        "input = torch.randn(1,3,5,5) #input image:3x5x5\n",
        "# 5x5 image: conv2d by 3x3 kernel stride=1 ==> 3x3\n",
        "print(\"Input:\")\n",
        "print(input)\n",
        "\n",
        "output = c1(input) #convolution 2d\n",
        "print(\"Output:\") #output feature map: 1x3x3\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ4UGOfM8d9h",
        "outputId": "9d605342-e1a5-47c0-bc62-c4b91b44c73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C1 weights: torch.Size([1, 3, 3, 3])\n",
            "Parameter containing:\n",
            "tensor([[[[-1.8834e-01,  4.1614e-02, -3.8510e-02],\n",
            "          [ 1.2210e-01, -1.3540e-01, -1.0622e-01],\n",
            "          [-7.7796e-05, -3.8769e-02,  1.5284e-01]],\n",
            "\n",
            "         [[ 1.5729e-01, -5.9668e-02, -1.3105e-02],\n",
            "          [ 1.5899e-01, -1.4271e-01,  8.8917e-02],\n",
            "          [ 4.8373e-02,  1.1290e-01, -5.4788e-02]],\n",
            "\n",
            "         [[ 4.2833e-02, -1.5918e-01, -1.0220e-01],\n",
            "          [ 5.5627e-02, -8.1208e-02, -1.1155e-01],\n",
            "          [ 1.6570e-01,  1.8301e-02,  5.2337e-02]]]], requires_grad=True)\n",
            "Input:\n",
            "tensor([[[[ 0.1660, -0.3331,  0.1576,  0.6322, -0.3814],\n",
            "          [ 1.7974, -0.2649,  0.9911, -0.6316,  0.2736],\n",
            "          [ 2.5167,  0.8752,  0.0710,  0.7472, -0.1425],\n",
            "          [-0.1590,  0.6963,  1.6341,  0.0304, -0.7747],\n",
            "          [ 0.1821,  1.0957,  0.8224, -0.1910,  2.2198]],\n",
            "\n",
            "         [[-2.1050, -0.1817, -0.9966, -0.9093, -0.3970],\n",
            "          [-0.9138, -0.1144,  0.5558,  1.1812, -2.1992],\n",
            "          [ 0.2768, -1.2767, -0.7687,  0.6505,  0.5564],\n",
            "          [-1.2369,  0.9544,  2.1611,  0.1965,  0.9808],\n",
            "          [-1.3898,  1.2520,  0.5537, -0.4433, -0.4679]],\n",
            "\n",
            "         [[ 1.0621, -0.5527, -2.0360, -1.8397, -0.6847],\n",
            "          [ 0.2989,  0.3777, -1.0983, -0.1609,  0.8843],\n",
            "          [-0.7594, -1.2210,  0.7876,  0.1352,  0.7752],\n",
            "          [ 0.3431,  1.7794, -1.0225, -0.6349, -0.4182],\n",
            "          [-0.0158,  0.0640,  0.4458, -2.3421, -1.8472]]]])\n",
            "Output:\n",
            "tensor([[[[ 0.0060,  0.4215,  0.1360],\n",
            "          [ 0.0921,  0.6180, -0.7730],\n",
            "          [-0.4569, -0.5263,  0.8570]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Conv2D:set Weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# create a 3 channel 5x5 imagae\n",
        "image = torch.ones((1,3,5,5))\n",
        "# create a conv2d:set bias False\n",
        "c1 = nn.Conv2d(in_channels=3,out_channels=1,kernel_size=3,stride=1,bias=False)\n",
        "# get weight of kernel\n",
        "w1 = c1.weight\n",
        "print(w1.shape,w1)\n",
        "# set weights\n",
        "c1.weight = nn.Parameter(torch.ones_like(w1),requires_grad=True) #default:True\n",
        "print(c1.weight.shape,c1.weight)\n",
        "fm = c1(image)\n",
        "print(\"Feature map:\")\n",
        "print(fm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu8mWE2lhaIk",
        "outputId": "69c29ad9-0829-4ee6-8ca4-49152789e33b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 3, 3]) Parameter containing:\n",
            "tensor([[[[-0.1278, -0.0867,  0.0600],\n",
            "          [-0.0684,  0.1427,  0.0456],\n",
            "          [ 0.0442,  0.1677,  0.0103]],\n",
            "\n",
            "         [[-0.1297, -0.0785, -0.0511],\n",
            "          [ 0.0303,  0.0210,  0.1788],\n",
            "          [ 0.1321,  0.0152, -0.1081]],\n",
            "\n",
            "         [[-0.0117,  0.0349,  0.0947],\n",
            "          [-0.0793,  0.0963,  0.0187],\n",
            "          [ 0.1436, -0.0716,  0.0755]]]], requires_grad=True)\n",
            "torch.Size([1, 3, 3, 3]) Parameter containing:\n",
            "tensor([[[[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]]]], requires_grad=True)\n",
            "Feature map:\n",
            "tensor([[[[27., 27., 27.],\n",
            "          [27., 27., 27.],\n",
            "          [27., 27., 27.]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "# create a filter\n",
        "filter = torch.ones((1,3,3))\n",
        "filter = filter*0.33333\n",
        "filter = torch.cat((filter,filter,filter), dim=0)\n",
        "filter = torch.unsqueeze(filter, 0)\n",
        "print(filter,filter.size()) #1,3,3,3\n",
        "# create a 3 channel 5x4 image\n",
        "c1 = torch.ones((1,5,5), dtype=torch.float32)\n",
        "c2 = torch.ones((1,5,5), dtype=torch.float32)*2\n",
        "c3 = torch.ones((1,5,5), dtype=torch.float32)*3\n",
        "print(c1)\n",
        "print(c2)\n",
        "print(c3)\n",
        "x = torch.cat((c1,c2,c3),dim=0)\n",
        "print(x)\n",
        "print(x.shape) #or .size()\n",
        "x = torch.unsqueeze(x,0)\n",
        "print(x)\n",
        "print(x.shape) #or .size()\n",
        "output = F.conv2d(x, filter, padding=0, groups=1)\n",
        "print(output, output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NV_a86WvBnl",
        "outputId": "64efcf8c-a94b-406a-9398-1a103db11ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.3333, 0.3333, 0.3333],\n",
            "          [0.3333, 0.3333, 0.3333],\n",
            "          [0.3333, 0.3333, 0.3333]],\n",
            "\n",
            "         [[0.3333, 0.3333, 0.3333],\n",
            "          [0.3333, 0.3333, 0.3333],\n",
            "          [0.3333, 0.3333, 0.3333]],\n",
            "\n",
            "         [[0.3333, 0.3333, 0.3333],\n",
            "          [0.3333, 0.3333, 0.3333],\n",
            "          [0.3333, 0.3333, 0.3333]]]]) torch.Size([1, 3, 3, 3])\n",
            "tensor([[[1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.]]])\n",
            "tensor([[[2., 2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2., 2.]]])\n",
            "tensor([[[3., 3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3., 3.]]])\n",
            "tensor([[[1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.]],\n",
            "\n",
            "        [[2., 2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2., 2.]],\n",
            "\n",
            "        [[3., 3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3., 3.]]])\n",
            "torch.Size([3, 5, 5])\n",
            "tensor([[[[1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 1.]],\n",
            "\n",
            "         [[2., 2., 2., 2., 2.],\n",
            "          [2., 2., 2., 2., 2.],\n",
            "          [2., 2., 2., 2., 2.],\n",
            "          [2., 2., 2., 2., 2.],\n",
            "          [2., 2., 2., 2., 2.]],\n",
            "\n",
            "         [[3., 3., 3., 3., 3.],\n",
            "          [3., 3., 3., 3., 3.],\n",
            "          [3., 3., 3., 3., 3.],\n",
            "          [3., 3., 3., 3., 3.],\n",
            "          [3., 3., 3., 3., 3.]]]])\n",
            "torch.Size([1, 3, 5, 5])\n",
            "tensor([[[[17.9998, 17.9998, 17.9998],\n",
            "          [17.9998, 17.9998, 17.9998],\n",
            "          [17.9998, 17.9998, 17.9998]]]]) torch.Size([1, 1, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`nn.Conv2d`的功能是对由多个输入平面组成的输入信号进行二维卷积。经过卷积后的图像尺寸计算公式为：`N = (W − F + 2P )/S+1`，其中`W`为原始图像大小，`F`为卷积核尺寸，`P`为`padding`大小，`S`是步长。`nn.ConvTranspose2d`的功能是进行反卷积操作，它可以将输入张量的尺寸扩大一定的倍数，并在扩大的同时填充一些值。与卷积层不同的是，反卷积层需要设置更多的超参数，例如输出大小、卷积核大小、步幅和填充。反卷积操作的输出尺寸计算公式为：`N = (W - 1) * S - 2P + F`，其中`W`为输入的尺寸，`F`为卷积核的大小，`P`是填充大小，`S`是步幅。需要注意的是，反卷积层的输出大小与卷积层的输入大小相同，因此反卷积层通常用于上采样操作。"
      ],
      "metadata": {
        "id": "8s9Wq_WmrI36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## conv2dTranspose example in Torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "# create input tensor\n",
        "input = torch.randn(1, 1, 5, 5)\n",
        "print(input.shape,\"\\n\",input)\n",
        "downsample = nn.Conv2d(1, 2, 3, stride=2, padding=1)\n",
        "upsample = nn.ConvTranspose2d(2, 1, 3, stride=2, padding=1)\n",
        "h = downsample(input)\n",
        "print(h.shape,\"\\n\",h)\n",
        "out = upsample(h, output_size=input.size()) #还原，最好指定output_size\n",
        "print(out.size(),\"\\n\",out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg2MTxJJixUh",
        "outputId": "4e1cdc49-0dd2-4f61-b90f-2a9413968766"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 5, 5]) \n",
            " tensor([[[[-0.4281,  0.4371, -0.3488,  0.6858,  1.7134],\n",
            "          [-1.0266,  0.0559, -0.5841,  0.4120, -1.0775],\n",
            "          [ 1.0788,  0.5567, -0.6487, -0.1753,  1.2828],\n",
            "          [-0.1243, -0.2857,  1.0360, -1.2395, -1.1020],\n",
            "          [ 0.2040,  1.0965,  2.2341,  1.2034,  1.3249]]]])\n",
            "torch.Size([1, 2, 3, 3]) \n",
            " tensor([[[[ 0.2368,  0.0629,  0.8463],\n",
            "          [ 0.3956, -0.6668,  0.4095],\n",
            "          [ 0.0515,  0.3718,  0.5039]],\n",
            "\n",
            "         [[-0.2204, -0.1264, -0.3743],\n",
            "          [-0.4845,  0.4811, -0.3077],\n",
            "          [-0.3359, -0.3673, -0.3157]]]], grad_fn=<ConvolutionBackward0>)\n",
            "torch.Size([1, 1, 5, 5]) \n",
            " tensor([[[[ 0.0045, -0.0778, -0.0047, -0.3624,  0.0907],\n",
            "          [-0.3496,  0.2674,  0.2607, -0.2546, -0.5288],\n",
            "          [-0.0420,  0.3170,  0.0223, -0.0847,  0.0154],\n",
            "          [-0.3502,  0.0426,  0.1645, -0.4188, -0.3992],\n",
            "          [-0.0717, -0.2022, -0.0112, -0.2929,  0.0337]]]],\n",
            "       grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    }
  ]
}