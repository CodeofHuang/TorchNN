{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNON50pgFTR8TMRe0dcxN0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sardear/TorchNN/blob/main/TorchNN_Hooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCp79nhB43_-",
        "outputId": "9fd7b799-0a20-4d90-84fc-20216ccecac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.grad = tensor([1., 2., 3., 4.])\n",
            "y.grad = tensor([1., 2., 3., 4.])\n",
            "w.grad = tensor([ 4.,  6.,  8., 10.])\n",
            "z.grad = tensor([1., 2., 3., 4.])\n",
            "Out = tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.Tensor([0,1,2,3]).requires_grad_()\n",
        "y = torch.Tensor([4,5,6,7]).requires_grad_()\n",
        "w = torch.Tensor([1,2,3,4]).requires_grad_()\n",
        "z = x + y\n",
        "\n",
        "z.retain_grad()\n",
        "out = w.matmul(z)\n",
        "out.retain_grad()\n",
        "out.backward()\n",
        "\n",
        "print(\"x.grad =\",x.grad) #see if requires_grad True\n",
        "print(\"y.grad =\",y.grad)\n",
        "print(\"w.grad =\",w.grad)\n",
        "print(\"z.grad =\",z.grad) #None (backward中间变量，不保留)\n",
        "print(\"Out =\",out.grad) #None (backward中间变量，不保留)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Add tensor hook\n",
        "def tensor_hook_fn(grad):\n",
        "  print(\" .grad= \",grad) #during backward\n",
        "  return\n",
        "\n",
        "x = torch.Tensor([0,1,2,3]).requires_grad_()\n",
        "y = torch.Tensor([4,5,6,7]).requires_grad_()\n",
        "w = torch.Tensor([1,2,3,4]).requires_grad_()\n",
        "z = x + y\n",
        "\n",
        "z.register_hook(tensor_hook_fn) #先打印out的结果，再打印z的结果\n",
        "out = w.matmul(z)\n",
        "out.register_hook(tensor_hook_fn) #先打印out的结果，再打印z的结果\n",
        "out.backward()\n",
        "\n",
        "print(\"x.grad =\",x.grad) #see if requires_grad True\n",
        "print(\"y.grad =\",y.grad)\n",
        "print(\"w.grad =\",w.grad)\n",
        "print(\"z.grad =\",z.grad) #None (backward中间变量，不保留gradient)\n",
        "print(\"Out =\",out.grad) #None (backward中间变量，不保留gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlgiitIZLEfw",
        "outputId": "3bb2234c-9a40-4d6b-f041-bbd0c885479a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " .grad=  tensor(1.)\n",
            " .grad=  tensor([1., 2., 3., 4.])\n",
            "x.grad = tensor([1., 2., 3., 4.])\n",
            "y.grad = tensor([1., 2., 3., 4.])\n",
            "w.grad = tensor([ 4.,  6.,  8., 10.])\n",
            "z.grad = None\n",
            "Out = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-4d0180b13c2d>:21: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
            "  print(\"z.grad =\",z.grad) #None (backward中间变量，不保留)\n",
            "<ipython-input-17-4d0180b13c2d>:22: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
            "  print(\"Out =\",out.grad) #None (backward中间变量，不保留)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# def tensor_hook_fn(g): #可以在hook发生时修改gradient\n",
        "#   g=g*2\n",
        "#   print(g)\n",
        "#   return g\n",
        "x = torch.Tensor ([0,1,2,3]).requires_grad_()\n",
        "y = torch.Tensor ([4,5,6,7]).requires_grad_()\n",
        "w = torch.Tensor ([1,2,3,4]).requires_grad_()\n",
        "z = x + y\n",
        "\n",
        "# z.register_hook(tensor_hook_fn)\n",
        "# out=z.retain_grad()\n",
        "\n",
        "z.register_hook(lambda n:n*2) #gradient *2，也可以在hook中使用lambda\n",
        "z.register_hook(lambda n:print(\"z.grad =\",n)) #print gradient\n",
        "\n",
        "out = w.matmul(z)\n",
        "out .backward()\n",
        "out.retain_grad()\n",
        "print(\"x.grad =\",x.grad)\n",
        "print(\"y.grad =\",y.grad)\n",
        "print(\"w.grad =\",w.grad)\n",
        "print(\"z.grad =\",z.grad) #None (backward中间变量，不保留gradient)\n",
        "print(\"Out =\",out.grad) #None (backward中间变量，不保留gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFuq6aIDNE-G",
        "outputId": "a3026e37-0003-47f0-bd95-b9a14cdacdfe"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z.grad = tensor([2., 4., 6., 8.])\n",
            "x.grad = tensor([2., 4., 6., 8.])\n",
            "y.grad = tensor([2., 4., 6., 8.])\n",
            "w.grad = tensor([ 4.,  6.,  8., 10.])\n",
            "z.grad = None\n",
            "Out = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-f6633768c490>:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
            "  print(\"z.grad =\",z.grad) #None (backward中间变量，不保留gradient)\n"
          ]
        }
      ]
    }
  ]
}